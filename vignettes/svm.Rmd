---
title: "Support Vector Machines (RStatistics.Net)"
author: "Kay-Uwe Kirstein"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

This vignettes describes the appliction of Support Vector Machine (SVM) on the clasification of sample data.
It is taken from the [SVM tutorial](http://rstatistics.net/support-vector-machines/) by [RStatisics.Net](http://rstatistics.net).


## Setup

Using the cats dataset from the MASS package:

```{r setup}
library(dplyr, warn.conflicts = FALSE)
library(ggplot2)
library(e1071)  # Misc Functions of the Department of Statistics, Probability Theory Group
                #(Formerly: E1071), TU Wien
```

```{r data_source}
data(cats, package = "MASS")
inputData <- data.frame(cats[, c (2,3)],
                        response = as.factor(cats$Sex)) %>% tbl_df # corce response to factor
print(inputData)
```


## Linear SVM

The key parameters passed to the `svm()` are _kernel_, _cost_ and _gamma_. Kernel is the type of SVM which could be linear, polynomial, radial or sigmoid. Cost is the cost function of constraint violation and gamma is a parameter used by all kernels except linear. There is a _type_ parameter that determines whether the model is used for regression, classification or novelty detection. But this need not be explicitly set as SVM will auto detect this based on the class of response variable being a factor or a continuous variable. So for classification problems, be sure to cast your response variable as a factor.

```{r linear_svm, fig.width=6, fig.asp=1}
svmfit <- svm(response ~ ., data = inputData,
              kernel = "linear", cost = 10, scale = FALSE) # linear SVM, scaling turned of

print(svmfit)

plot(svmfit, inputData)

compareTable <- table(inputData$response, predict(svmfit))

mean(inputData$response != predict(svmfit)) # misclassification factor
```


## Radial SVM

The radial basis function, a popular kernel function can be used by setting the _kernel_ parameter as “radial”. When a "radial" kernel is used the resulting hyperplane need not be a line anymore. A curved region of separation is usually defined to demarcate the separation between classes, often leading to higher accuracy within the training data.

```{r radial_svm, fig.width=6, fig.asp=1}
svmfit <- svm(response ~ ., data = inputData,
              kernel = "radial", cost = 10, scale = FALSE) # radial SVM, scaling turned off

print(svmfit)

plot(svmfit, inputData)

compareTable <- table(inputData$response, predict(svmfit))

mean(inputData$response != predict(svmfit)) # misclassification factor
```


## Find the Optimal Parameters

### Obtain Training- and Test-Dataset

```{r training_set}
set.seed(100) # for reproducible results

rowIndices <- 1:nrow(inputData)
sampleSize <- 0.8 * length(rowIndices) # take 80% of samples as training set

trainingRows <- sample(rowIndices, sampleSize)
trainingData <- inputData[trainingRows, ] # training data
testData <- inputData[-trainingRows, ]    # test data
```

Alternatively, the `sample_frac()` (or `sample_n()`) method from the _dplyr_ package can be used:

```{r training_set_dplyr}
set.seed(100) # for reproducible results

# add an row index
inputData <- mutate(inputData, Idx = seq_along(Bwt))

trainingData <- sample_frac(inputData, 0.8)   # training data
testData <- setdiff(inputData, trainingData)  # test data

# clean-up
inputData <- select(inputData, -Idx)
trainingData <- select(trainingData, -Idx)
testData <- select(testData, -Idx)
```


### Tune SVM Parameter

You can find the optimal parameters for the svm() using the tune.svm() function.

```{r tune_svm}
tuned <- tune.svm(response ~ ., data = trainingData,
                  gamma = 10^(-6:-1), cost = 10^(1:2)) # tune

summary(tuned)
```

```{r optimal_svm, fig.width=6, fig.asp=1}
svmfit <- svm(response ~ ., data = trainingData,
              kernel = "radial", gamma = 0.01, cost = 100,
              scale = FALSE) # radial SVM, scaling turned of

print(svmfit)

plot(svmfit, trainingData)

# apply testData
compareTable <- table(testData$response, predict(svmfit, testData))

mean(testData$response != predict(svmfit, testData)) # misclassification factor
```


## Grid Plot

A 2-coloured grid plot, makes is visually clear which regions of the plot is designated to which class of response by the SVM classifier. In the below example, the data points are plotted against such a grid and the support vectors points are marked by a tilted square around the data points. Obviously, in this case, there are many constraint violations marked by the boundary cross-overs, but these are weighted down by the SVM internally.

### Traditionally with Plot/Point

```{r grid_plot, fig.width=6, fig.asp=1}
n_points_in_grid = 60 # num grid points in a line

x_axis_range <- range(inputData[, 2]) # range of X axis
y_axis_range <- range(inputData[, 1]) # range of Y axis

X_grid_points <- seq(from=x_axis_range[1], to=x_axis_range[2],
                     length=n_points_in_grid) # grid points along x-axis
Y_grid_points <- seq(from=y_axis_range[1], to=y_axis_range[2],
                     length=n_points_in_grid) # grid points along y-axis

all_grid_points <- expand.grid (X_grid_points, Y_grid_points) # generate all grid points
names(all_grid_points) <- c("Hwt", "Bwt") # rename

all_points_predited <- predict(svmfit, all_grid_points) # predict for all points in grid
color_array <-
  c("red", "blue")[as.numeric(all_points_predited)] # colors for all points based on predictions

plot(all_grid_points, col=color_array, pch=20, cex=0.25) # plot all grid points

points(x=trainingData$Hwt, y=trainingData$Bwt,
       col=c("red", "blue")[as.numeric(trainingData$response)], pch=19) # plot data points

points(trainingData[svmfit$index, c (2, 1)], pch=5, cex=1.5) # plot support vectors
```


### With GGPlot

```{r grid_ggplot, fig.width=6, fig.asp=1}
n_points_in_grid = 60 # num grid points in a line

y_axis_range <- range(inputData$Bwt) # range of Y axis
x_axis_range <- range(inputData$Hwt) # range of X axis

grid_points <- setNames(expand.grid(seq(x_axis_range[1], x_axis_range[2],
                                        length.out = n_points_in_grid),
                                    seq(y_axis_range[1], y_axis_range[2],
                                        length.out = n_points_in_grid)),
                        c("Hwt", "Bwt"))
grid_points <- mutate(grid_points, response = predict(svmfit, grid_points))

grid_plot <- ggplot() +
  geom_point(data = grid_points, aes(x = Hwt, y = Bwt, col = response),
             size = 0.8) +
  geom_point(data = trainingData, aes(x = Hwt, y = Bwt, shape = response)) +
  scale_shape_manual(values = c(1, 2)) +
  ggtitle("Grid Plot of Optimized SVM Model")

print(grid_plot)
```

