---
title: "Support Vector Machines (RStatistics.Net)"
author: "Kay-Uwe Kirstein"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

This vignettes describes the appliction of Support Vector Machine (SVM) on the clasification of sample data.
It is taken from the [SVM tutorial](http://rstatistics.net/support-vector-machines/) by [RStatisics.Net](http://rstatistics.net).


## Setup

Using the cats dataset from the MASS package:

```{r setup}
library(e1071)  # Misc Functions of the Department of Statistics, Probability Theory Group
                #(Formerly: E1071), TU Wien

data(cats, package = "MASS")
inputData <- data.frame(cats[, c (2,3)],
                        response = as.factor(cats$Sex)) # setup response as factor
```


## Linear SVM

```{r linear_svm, fig.width=6, fig.asp=1}
svmfit <- svm(response ~ ., data = inputData,
              kernel = "linear", cost = 10, scale = FALSE) # linear SVM, scaling turned of

print(svmfit)

plot(svmfit, inputData)

compareTable <- table(inputData$response, predict(svmfit))

mean(inputData$response != predict(svmfit)) # misclassification factor
```


## Radial SVM

```{r radial_svm, fig.width=6, fig.asp=1}
svmfit <- svm(response ~ ., data = inputData,
              kernel = "radial", cost = 10, scale = FALSE) # radial SVM, scaling turned of

print(svmfit)

plot(svmfit, inputData)

compareTable <- table(inputData$response, predict(svmfit))

mean(inputData$response != predict(svmfit)) # misclassification factor
```


## Find the Optimal Parameters

```{r training_set}
set.seed(100) # for reproducible results

rowIndices <- 1:nrow(inputData)
sampleSize <- 0.8 * length(rowIndices) # take 80% of samples as training set

trainingRows <- sample(rowIndices, sampleSize)
trainingData <- inputData[trainingRows, ] # training data
testData <- inputData[-trainingRows, ]    # test data
```

```{r tune_svm}
tuned <- tune.svm(response ~ ., data = trainingData,
                  gamma = 10^(-6:-1), cost = 10^(1:2)) # tune

summary(tuned)
```

```{r optimal_svm, fig.width=6, fig.asp=1}
svmfit <- svm(response ~ ., data = trainingData,
              kernel = "radial", gamma = 0.01, cost = 100,
              scale = FALSE) # radial SVM, scaling turned of

print(svmfit)

plot(svmfit, trainingData)

compareTable <- table(testData$response, predict(svmfit, testData))

mean(testData$response != predict(svmfit, testData)) # misclassification factor
```


## Grid Plot

### Traditionally with Plot/Point

```{r grid_plot, fig.width=6, fig.asp=1}
n_points_in_grid = 60 # num grid points in a line

x_axis_range <- range(inputData[, 2]) # range of X axis
y_axis_range <- range(inputData[, 1]) # range of Y axis

X_grid_points <- seq(from=x_axis_range[1], to=x_axis_range[2],
                     length=n_points_in_grid) # grid points along x-axis
Y_grid_points <- seq(from=y_axis_range[1], to=y_axis_range[2],
                     length=n_points_in_grid) # grid points along y-axis

all_grid_points <- expand.grid (X_grid_points, Y_grid_points) # generate all grid points
names(all_grid_points) <- c("Hwt", "Bwt") # rename

all_points_predited <- predict(svmfit, all_grid_points) # predict for all points in grid
color_array <-
  c("red", "blue")[as.numeric(all_points_predited)] # colors for all points based on predictions

plot(all_grid_points, col=color_array, pch=20, cex=0.25) # plot all grid points

points(x=trainingData$Hwt, y=trainingData$Bwt,
       col=c("red", "blue")[as.numeric(trainingData$response)], pch=19) # plot data points

points(trainingData[svmfit$index, c (2, 1)], pch=5, cex=1.5) # plot support vectors
```


### With GGPlot

```{r grid_ggplot, fig.width=6, fig.asp=1}


```

